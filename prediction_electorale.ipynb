{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvH1DWirZFUj"
   },
   "source": [
    "#Traitement des données de démographie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "XpRwxIEUL8oR",
    "outputId": "66aa6b46-8e41-4488-c68a-831abce02752"
   },
   "outputs": [],
   "source": [
    "demographie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T06:49:57.514469Z",
     "start_time": "2024-05-14T06:49:57.504647Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "uaurRFNeLq_H",
    "outputId": "87345275-64fe-488b-8516-85a615a6f8be"
   },
   "outputs": [],
   "source": [
    "#Choisissons les dates auxquelles il y'a eu des élections\n",
    "colonne_a_gardé=['LIBGEO','PSDC1999']\n",
    "Demographie =demographie[colonne_a_gardé].copy()\n",
    "for i in ['PMUN2007','PMUN2012','PMUN2017','PMUN2021']:\n",
    "    colonne=['LIBGEO',i]\n",
    "    tableau = demographie[colonne]\n",
    "    tableau.columns=colonne_a_gardé\n",
    "    Demographie = pd.concat([Demographie, tableau], ignore_index=True)\n",
    "Demographie.columns=['Libellé de la commune','Population']\n",
    "Demographie['Date']=[2002] * 34947 + [2007] * 34947 + [2012] * 34947+ [2017] * 34947 + [2022] *34947\n",
    "Demographie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avGsOoIaLTHu"
   },
   "source": [
    "#Traitement des données d'education et generation des echantillons pour chaque commune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HR3BOq7yPRR",
    "outputId": "053bff80-96ae-40a8-fa37-fa5c259b8187"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chemin vers votre fichier Excel\n",
    "chemin_fichier = r\"C:\\Users\\sebeo\\OneDrive\\Bureau\\DATASET\\donnés d'education.xlsx\"\n",
    "# Lire le fichier Excel\n",
    "education = pd.read_excel(chemin_fichier)\n",
    "\n",
    "# Filtrons les lignes pour les années spécifiées auquels il y'a eu des elections\n",
    "annees_a_garder = [2002, 2007, 2012, 2017, 2022]\n",
    "df_filtre = education[education['Année'].isin(annees_a_garder)]\n",
    "\n",
    "# Réinitialiser l'index\n",
    "df_filtre = df_filtre.reset_index(drop=True)\n",
    "\n",
    "# Afficher le DataFrame filtré\n",
    "df_filtre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEe4Csx_gb0j"
   },
   "outputs": [],
   "source": [
    "def generate_normal_samples(mu_values, sigma_values, size):\n",
    "    \"\"\"\n",
    "    Génère des échantillons suivant une distribution normale.\n",
    "\n",
    "    Args:\n",
    "        mu_values (float): Moyenne de la distribution normale.\n",
    "        sigma_values (float): Écart-type de la distribution normale.\n",
    "        size (int or tuple of ints): Taille de l'échantillon à générer.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Échantillons suivant une distribution normale.\n",
    "    \"\"\"\n",
    "    samples = np.random.normal(loc=mu_values, scale=sigma_values, size=size)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSRApuT9RANY"
   },
   "source": [
    "*pour generer les taux de reussite en se basant sur le nombre de personnes dans chaque commune et en supposant que la distribution suit une loi normal, nous allons, tout abord determiner les poids qui seront affecter à chaque commune afin d'avoir une génération de données représentative de la commune en question*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUjL_ROuKXgX"
   },
   "outputs": [],
   "source": [
    "Dates=[2002,2007,2012,2017,2022]\n",
    "listes_des_poids=[]\n",
    "\n",
    "for date in Dates:\n",
    "  # Filtrer les lignes où l'année est égale à 2002\n",
    "  population = Demographie[Demographie['Date'] == date]\n",
    "\n",
    " # Calculer la somme de la population pour l'année 2002\n",
    "  somme_population = population['Population'].sum()\n",
    "\n",
    "  listes_des_poids.extend(population['Population']/somme_population)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1UJjD5KYQT1",
    "outputId": "20fb4343-af6f-4cfb-a832-cfd2080112b8"
   },
   "outputs": [],
   "source": [
    "listes_des_poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5cYYXTpzqL_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Créer une liste pour stocker les échantillons générés\n",
    "Bac_Tech_list = []\n",
    "Bac_Gen_list = []\n",
    "Bac_Pro_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    # Calculer l'écart-type comme la racine carrée de la variance\n",
    "    sigma_values = 1\n",
    "    size = 34947\n",
    "\n",
    "    # Utiliser sigma_values pour générer des échantillons suivant une loi normale\n",
    "    Bac_Tech = generate_normal_samples(df_filtre['Baccalauréat technologique'][i], sigma_values, size)\n",
    "    Bac_Gen = generate_normal_samples(df_filtre['Baccalauréat général'][i], sigma_values, size)\n",
    "    Bac_Pro = generate_normal_samples(df_filtre['Baccalauréat professionnel'][i], sigma_values, size)\n",
    "\n",
    "    # Ajouter les échantillons générés à la liste\n",
    "    Bac_Tech_list.extend(Bac_Tech)\n",
    "    Bac_Gen_list.extend(Bac_Gen)\n",
    "    Bac_Pro_list.extend(Bac_Pro)\n",
    "\n",
    "# Ajuster les échantillons en fonction de la population de chaque commune\n",
    "Bac_Tech_list_weighted = [Bac_Tech_list[i] * listes_des_poids[i] for i in range(len(Bac_Tech_list))]\n",
    "Bac_Gen_list_weighted = [Bac_Gen_list[i] * listes_des_poids[i] for i in range(len(Bac_Gen_list))]\n",
    "Bac_Pro_list_weighted = [Bac_Pro_list[i] * listes_des_poids[i] for i in range(len(Bac_Pro_list))]\n",
    "\n",
    "# Créer une liste d'années pondérées pour chaque échantillon\n",
    "Année = [df_filtre['Année'][i] for i in range(len(df_filtre['Année']))] * 34947\n",
    "\n",
    "# Créer le DataFrame en utilisant les listes de données ajustées\n",
    "Niveau_education = pd.DataFrame({\n",
    "    'Libellé de la commune':Demographie['Libellé de la commune'],\n",
    "    'Baccalauréat général': Bac_Gen_list_weighted,\n",
    "    'Baccalauréat technologique': Bac_Tech_list_weighted,\n",
    "    'Baccalauréat professionnel': Bac_Pro_list_weighted,\n",
    "    'Date': Année\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3780fZc3yeQ",
    "outputId": "7d88c9ac-6701-4f81-e01e-ca0f8dc71a88"
   },
   "outputs": [],
   "source": [
    "Niveau_education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfHqIXM2tqCb"
   },
   "source": [
    "# Traitement des Données de revenu des menages par communes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIHjnorvxyQf"
   },
   "source": [
    " *On constate qu'a apartir de la colonne MED12 LE nombre d'elements manquants est très elevé; ce qui nous amene a ne garder que les 5 premieres colonnes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0zUiJp6GnOB",
    "outputId": "104bc6ec-4fcc-4d77-bb51-670bc6a35cb5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Liste des noms de fichiers Excel\n",
    "fichiers_excel = [\n",
    "    r\"C:\\Users\\sebeo\\OneDrive\\Bureau\\DATASET\\revenu\\revenu 2012.xlsx\",\n",
    "    r\"C:\\Users\\sebeo\\OneDrive\\Bureau\\DATASET\\revenu\\revenu 2017.xlsx\",\n",
    "    r\"C:\\Users\\sebeo\\OneDrive\\Bureau\\DATASET\\revenu\\revenu 2022.xlsx\",\n",
    "]\n",
    "\n",
    "# Initialisation d'une liste pour stocker les nouveaux DataFrames et les dates correspondantes\n",
    "revenu_dataframes = []\n",
    "dates = [2012, 2017, 2022]\n",
    "\n",
    "# Parcourir chaque fichier Excel\n",
    "for fichier, date in zip(fichiers_excel, dates):\n",
    "\n",
    "    # Lire le fichier Excel\n",
    "    revenu = pd.read_excel(fichier)\n",
    "\n",
    "    # Sélectionner les cinq premières colonnes\n",
    "    revenu = revenu.iloc[:, :5]\n",
    "    revenu.columns=['CODGEO', 'LIBGEO', 'NBMENFISC','NBPERSMENFISC12','MED']\n",
    "\n",
    "    # Exclure la colonne 'LIBGEO' du processus\n",
    "    for col in revenu.columns:\n",
    "        if col != 'LIBGEO':\n",
    "            # Conversion de la colonne en numérique, en remplaçant les valeurs non numériques par NaN\n",
    "            revenu[col] = pd.to_numeric(revenu[col], errors='coerce')\n",
    "\n",
    "            # Calcul de la médiane des valeurs numériques dans la colonne\n",
    "            median_value = revenu[col].median()\n",
    "\n",
    "            # Remplacement des valeurs non numériques par la médiane calculée\n",
    "            revenu[col].fillna(median_value, inplace=True)\n",
    "\n",
    "    # Ajout de la colonne 'Date' avec une année donnée\n",
    "    revenu['Date'] = date\n",
    "\n",
    "    # Ajouter le DataFrame au liste\n",
    "    revenu_dataframes.append(revenu)\n",
    "\n",
    "# Concaténer tous les DataFrames en un seul\n",
    "revenu_final = pd.concat(revenu_dataframes, ignore_index=True)\n",
    "\n",
    "revenu_final.columns=['CODGEO', 'Libellé de la commune', 'NBMENFISC','NBPERSMENFISC12','MED', 'Date']\n",
    "\n",
    "# Afficher les premières lignes du DataFrame final\n",
    "revenu_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1xkFPrWmE7p"
   },
   "source": [
    "\n",
    "*   'CODGEO' : le code associer à chaque commune\n",
    "*  'NBMENFISC' : Nombre de ménages fiscaux\n",
    "\n",
    "*   'NBPERSMENFISC12' : Nombre de personnes dans les ménages fiscaux\n",
    "*   'MED' : Médiane du niveau de vie (€)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVQFTw9F1HLa"
   },
   "source": [
    "# Visualisation et interprétations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tF7BMx2ARSNF",
    "outputId": "0cff1cc6-b274-42d5-d4e2-3f4673371dba"
   },
   "outputs": [],
   "source": [
    "# Grouper les données par le nom de la commune et calculer la somme des populations\n",
    "communes_grouped = Demographie.groupby('Libellé de la commune')['Population'].sum()\n",
    "\n",
    "# Trier les communes par ordre décroissant de population et sélectionner les cinq premières\n",
    "top_5_communes = communes_grouped.sort_values(ascending=False).head(5)\n",
    "\n",
    "# Afficher les cinq communes les plus peuplées\n",
    "print(top_5_communes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8xXG1xGGSJz4",
    "outputId": "b14d408b-a49c-4bad-99ee-f3aaae1486fb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Liste des cinq communes les plus peuplées\n",
    "top_5_communes_list = top_5_communes.index.tolist()\n",
    "\n",
    "# Filtrer les données pour les cinq communes les plus peuplées\n",
    "top_5_communes_data = Demographie[Demographie['Libellé de la commune'].isin(top_5_communes_list)]\n",
    "\n",
    "# Création d'une figure et des axes pour les sous-graphiques\n",
    "fig, axs = plt.subplots(5, 1, figsize=(10, 20), sharex=True)\n",
    "\n",
    "# Parcours des cinq communes\n",
    "for i, commune in enumerate(top_5_communes_list):\n",
    "    # Sélection des données pour la commune spécifique\n",
    "    commune_data = top_5_communes_data[top_5_communes_data['Libellé de la commune'] == commune]\n",
    "    # Tracé de l'histogramme de la population pour chaque année\n",
    "    axs[i].bar(commune_data['Date'], commune_data['Population'])\n",
    "    # Ajout de titre pour chaque sous-graphique\n",
    "    axs[i].set_title(commune)\n",
    "    axs[i].set_ylabel('Population')\n",
    "    axs[i].set_xlabel('Date')\n",
    "\n",
    "# Ajout de titre pour l'ensemble du graphique\n",
    "fig.suptitle('Évolution de la population des cinq communes les plus peuplées (2002-2022)')\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vEtzy7h2ZfPe",
    "outputId": "a742c38b-3d24-42ac-e490-364b40740722"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Liste des années\n",
    "annees = [2012, 2017, 2022]\n",
    "\n",
    "# Liste des cinq communes\n",
    "top_5_communes_list = ['Marseille', 'Lyon', 'Toulouse', 'Nice', 'Nantes']\n",
    "\n",
    "# Création d'une figure et des axes pour les sous-graphiques\n",
    "fig, axs = plt.subplots(5, len(annees), figsize=(20, 20), sharex=True, sharey=True)\n",
    "\n",
    "# Parcours des années\n",
    "for j, annee in enumerate(annees):\n",
    "    # Parcours des cinq communes\n",
    "    for i, commune in enumerate(top_5_communes_list):\n",
    "        # Sélection des données pour la commune spécifique et l'année spécifique\n",
    "        commune_data = resultat_final[(resultat_final['Libellé de la commune'] == commune) & (resultat_final['Date'] == annee)]\n",
    "        # Regroupement des données par candidat et calcul du nombre total de voix pour chaque candidat\n",
    "        votes_par_candidat = commune_data.groupby('Nom')['Voix'].sum()\n",
    "        # Création du diagramme circulaire\n",
    "        axs[i, j].pie(votes_par_candidat, labels=votes_par_candidat.index, autopct='%1.1f%%', startangle=140)\n",
    "        # Ajout de titre pour chaque sous-graphique\n",
    "        axs[i, j].set_title(f\"{commune} - {annee}\")\n",
    "\n",
    "# Ajout de titre pour l'ensemble du graphique\n",
    "fig.suptitle('Distribution des votes par candidat dans les cinq communes les plus peuplées (2012, 2017, 2022)')\n",
    "\n",
    "# Ajustement de la disposition des sous-graphiques\n",
    "plt.tight_layout()\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pw32I5lub_yl",
    "outputId": "5d296f71-01e7-4bca-e5eb-5c21864b5660"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Liste des années\n",
    "annees = [2012, 2017, 2022]\n",
    "\n",
    "# Colonnes à tracer\n",
    "colonnes = ['NBMENFISC', 'NBPERSMENFISC12', 'MED']\n",
    "\n",
    "# Libellés des communes spécifiques\n",
    "communes_specifiques = ['Marseille', 'Lyon', 'Toulouse', 'Nice', 'Nantes']\n",
    "\n",
    "# Boucle sur les libellés de commune spécifiques\n",
    "for commune in communes_specifiques:\n",
    "    # Création d'une figure et d'axes pour chaque commune\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Titre du graphique avec le nom de la commune\n",
    "    ax.set_title(f'Évolution des différentes colonnes de revenu pour la commune de {commune}')\n",
    "\n",
    "    # Boucle sur les colonnes\n",
    "    for i, colonne in enumerate(colonnes):\n",
    "        # Initialisation des données pour la colonne\n",
    "        donnees_colonne = []\n",
    "        # Parcours des années\n",
    "        for annee in annees:\n",
    "            # Sélection des données pour l'année spécifique, la colonne spécifique et le libellé de commune spécifique\n",
    "            annee_data = revenu_final[(revenu_final['Date'] == annee) & (revenu_final['Libellé de la commune'] == commune)]\n",
    "            valeurs_colonne = annee_data[colonne]\n",
    "            # Calcul de la moyenne pour l'année spécifique\n",
    "            moyenne_valeurs = np.mean(valeurs_colonne)\n",
    "            # Ajout de la moyenne calculée aux données de la colonne\n",
    "            donnees_colonne.append(moyenne_valeurs)\n",
    "        # Tracé de la courbe pour la colonne\n",
    "        ax.plot(annees, donnees_colonne, label=colonne)\n",
    "\n",
    "    # Étiquettes pour les axes\n",
    "    ax.set_xlabel('Année')\n",
    "    ax.set_ylabel('Valeur moyenne')\n",
    "\n",
    "    # Légende\n",
    "    ax.legend()\n",
    "\n",
    "    # Affichage du graphique pour la commune spécifique\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gzfQGx7DwI9o",
    "outputId": "0b63be90-a6ff-4450-eb02-1631b9fe2f85"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Colonnes à tracer\n",
    "colonnes = ['Baccalauréat général', 'Baccalauréat technologique', 'Baccalauréat professionnel']\n",
    "\n",
    "# Libellés des communes spécifiques\n",
    "communes_specifiques = ['Marseille', 'Lyon', 'Toulouse', 'Nice', 'Nantes']\n",
    "\n",
    "# Création d'une figure et d'axes\n",
    "fig, axs = plt.subplots(len(communes_specifiques), len(colonnes), figsize=(15, 15))\n",
    "\n",
    "# Boucle sur les libellés de commune spécifiques\n",
    "for i, commune in enumerate(communes_specifiques):\n",
    "    # Sélection des données pour la commune spécifique\n",
    "    commune_data = Niveau_education[Niveau_education['Libellé de la commune'] == commune]\n",
    "\n",
    "    # Boucle sur les colonnes\n",
    "    for j, colonne in enumerate(colonnes):\n",
    "        # Données à tracer\n",
    "        donnees = commune_data[colonne]\n",
    "\n",
    "        # Tracé de l'histogramme\n",
    "        axs[i, j].hist(donnees, bins=10, color='skyblue', edgecolor='black')\n",
    "\n",
    "        # Ajout de titres et d'étiquettes\n",
    "        axs[i, j].set_title(f'{commune} - {colonne}')\n",
    "        axs[i, j].set_xlabel('Taux de réussite')\n",
    "        axs[i, j].set_ylabel('Fréquence')\n",
    "        axs[i, j].grid(True)\n",
    "\n",
    "# Ajustement de l'espacement entre les sous-graphiques\n",
    "plt.tight_layout()\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1YFnlP8047V"
   },
   "source": [
    "#traitement des données d'elections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNU6pylzyHaa"
   },
   "source": [
    "traitement des données du premier tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "WRdPeDpg1DTJ",
    "outputId": "4e6aaf93-1057-4d75-f3a0-c586d387c016"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Liste des noms de fichiers Excel\n",
    "fichiers_excel = [\n",
    "    r\"C:\\Users\\sebeo\\OneDrive\\Bureau\\DATASET\\tr 1\\2002 tr 1.xls\",\n",
    "    r\"C:\\Users\\sebeo\\OneDrive\\Bureau\\DATASET\\tr 1\\2007 tr 1.xlsx\",\n",
    "    r\"C:\\Users\\sebeo\\OneDrive\\Bureau\\DATASET\\tr 1\\2012 tr 1.xlsx\",\n",
    "    r\"C:\\Users\\sebeo\\OneDrive\\Bureau\\DATASET\\tr 1\\2017 tr 1.xls\",\n",
    "    r\"C:\\Users\\sebeo\\OneDrive\\Bureau\\DATASET\\tr 1\\2022 tr 1.xls\"\n",
    "]\n",
    "\n",
    "# Initialisation d'une liste pour stocker les nouveaux DataFrames et les dates correspondantes\n",
    "nouveaux_dataframes = []\n",
    "dates = [2002,2007,2012,2017,2022]\n",
    "\n",
    "# Parcourir chaque fichier Excel\n",
    "for fichier, date in zip(fichiers_excel, dates):\n",
    "\n",
    "    # Charger le fichier Excel dans un DataFrame pandas\n",
    "    df = pd.read_excel(fichier)\n",
    "\n",
    "    # Colonnes à garder\n",
    "    colonnes_a_garder = ['Libellé de la commune','Inscrits','Abstentions','Votants','Exprimés', 'Sexe', 'Nom', 'Prénom', 'Voix', '% Voix/Ins', '% Voix/Exp']\n",
    "\n",
    "    # Création d'un nouveau DataFrame avec les colonnes sélectionnées\n",
    "    nouveau_df = df[colonnes_a_garder].copy()\n",
    "\n",
    "    # Ajout des valeurs des colonnes supplémentaires\n",
    "    x = int((df.shape[1] - 13) / 6)\n",
    "    for j in tqdm(range(1, x)):\n",
    "        colonnes = ['Libellé de la commune','Inscrits','Abstentions','Votants','Exprimés', f'Sexe.{j}', f'Nom.{j}', f'Prénom.{j}', f'Voix.{j}', f'% Voix/Ins.{j}', f'% Voix/Exp.{j}']\n",
    "        tableau = df[colonnes]\n",
    "        tableau.columns=colonnes_a_garder\n",
    "        nouveau_df = pd.concat([nouveau_df, tableau], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Ajout de la colonne 'Date' avec une année données\n",
    "    nouveau_df['Date'] = pd.to_datetime(str(date), format='%Y').year\n",
    "    # Réinitialisation de l'index\n",
    "    nouveau_df.reset_index(drop=True, inplace=True)\n",
    "    # Ajouter le nouveau DataFrame et la date correspondante à la liste\n",
    "    nouveaux_dataframes.append(nouveau_df)\n",
    "\n",
    "# Concaténer tous les nouveaux DataFrames en un seul DataFrame\n",
    "resultat_final = pd.concat(nouveaux_dataframes, ignore_index=True)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame final\n",
    "resultat_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "1WuiOxDs047X",
    "outputId": "afd69b9c-09af-48cc-bb0a-3ebb8ba61d0e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Liste des noms de fichiers Excel\n",
    "fichiers_excel = [\n",
    "    r\"C:\\Users\\sebeo\\OneDrive\\Bureau\\DATASET\\tr 2\\2002 tr 2.xls\",\n",
    "    r\"C:\\Users\\sebeo\\OneDrive\\Bureau\\DATASET\\tr 2\\2007 tr 2.xlsx\",\n",
    "    r\"C:\\Users\\sebeo\\OneDrive\\Bureau\\DATASET\\tr 2\\2012 tr 2.xlsx\",\n",
    "    r\"C:\\Users\\sebeo\\OneDrive\\Bureau\\DATASET\\tr 2\\2017 tr 2.xls\",\n",
    "    r\"C:\\Users\\sebeo\\OneDrive\\Bureau\\DATASET\\tr 2\\2022 tr 2.xlsx\"\n",
    "]\n",
    "\n",
    "# Initialisation d'une liste pour stocker les nouveaux DataFrames et les dates correspondantes\n",
    "nouveaux_dataframes = []\n",
    "dates = [2002,2007,2012,2017,2022]\n",
    "\n",
    "# Parcourir chaque fichier Excel\n",
    "for fichier, date in zip(fichiers_excel, dates):\n",
    "\n",
    "    # Charger le fichier Excel dans un DataFrame pandas\n",
    "    df = pd.read_excel(fichier)\n",
    "\n",
    "    # Colonnes à garder\n",
    "    colonnes_a_garder = ['Libellé de la commune','Inscrits','Abstentions','Votants','Exprimés', 'Sexe', 'Nom', 'Prénom', 'Voix', '% Voix/Ins', '% Voix/Exp']\n",
    "\n",
    "    # Création d'un nouveau DataFrame avec les colonnes sélectionnées\n",
    "    nouveau_df = df[colonnes_a_garder].copy()\n",
    "\n",
    "    # Ajout des valeurs des colonnes supplémentaires\n",
    "    x = 2\n",
    "    for j in tqdm(range(1, x)):\n",
    "        colonnes = ['Libellé de la commune','Inscrits','Abstentions','Votants','Exprimés', f'Sexe.{j}', f'Nom.{j}', f'Prénom.{j}', f'Voix.{j}', f'% Voix/Ins.{j}', f'% Voix/Exp.{j}']\n",
    "        tableau = df[colonnes]\n",
    "        tableau.columns=colonnes_a_garder\n",
    "        nouveau_df = pd.concat([nouveau_df, tableau], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Ajout de la colonne 'Date' avec une année données\n",
    "    nouveau_df['Date'] = pd.to_datetime(str(date), format='%Y').year\n",
    "    # Réinitialisation de l'index\n",
    "    nouveau_df.reset_index(drop=True, inplace=True)\n",
    "    # Ajouter le nouveau DataFrame et la date correspondante à la liste\n",
    "    nouveaux_dataframes.append(nouveau_df)\n",
    "\n",
    "# Concaténer tous les nouveaux DataFrames en un seul DataFrame\n",
    "tour2_result = pd.concat(nouveaux_dataframes, ignore_index=True)\n",
    "\n",
    "tour2_result.columns= ['Libellé de la commune', 'Inscrits2',\n",
    "       'Abstentions2', 'Votants2', 'Exprimés2', 'Sexe', 'Nom', 'Prénom', 'Voix2',\n",
    "       '% Voix/Ins2', '% Voix/Exp2', 'Date']\n",
    "# Afficher les premières lignes du DataFrame final\n",
    "tour2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "2EyhBJvZhnnF",
    "outputId": "66b5185b-dd8f-46ed-dbac-b1c60adae77c"
   },
   "outputs": [],
   "source": [
    "# Fusionner les datasets pour le tour 1 et celui du tour 2 en utilisant les colonnes spécifiées et en conservant toutes les lignes\n",
    "elections= tour2_result.merge(resultat_final, on=['Libellé de la commune', 'Prénom', 'Nom', 'Sexe', 'Date'], how='outer')\n",
    "\n",
    "# Remplacer les valeurs NaN par zéro\n",
    "elections.fillna(0, inplace=True)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame fusionné\n",
    "elections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2CZ8fEd1OBV"
   },
   "source": [
    "#Fusion des Datasets déjà traités\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwPoiDvy0dIB",
    "outputId": "a09da0ed-8d35-4e7b-97c1-11b29b40ce23"
   },
   "outputs": [],
   "source": [
    "# Importer la bibliothèque pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Définir les dates pour lesquelles vous souhaitez fusionner les données\n",
    "dates = [2022]\n",
    "\n",
    "# Filtrer les ensembles de données originaux pour les dates spécifiées\n",
    "elections_filtered = elections[elections['Date'].isin(dates)]\n",
    "niveau_education_filtered = Niveau_education[Niveau_education['Date'].isin(dates)]\n",
    "revenu_final_filtered = revenu_final[revenu_final['Date'].isin(dates)]\n",
    "demographie_filtered = Demographie[Demographie['Date'].isin(dates)]\n",
    "\n",
    "# Fusionner les ensembles de données en utilisant les colonnes communes\n",
    "Dataset = pd.merge(elections_filtered, niveau_education_filtered, on=['Libellé de la commune', 'Date']) \\\n",
    "             .merge(revenu_final_filtered, on=['Libellé de la commune', 'Date']) \\\n",
    "             .merge(demographie_filtered, on=['Libellé de la commune', 'Date'])\n",
    "\n",
    "# Afficher le jeu de données fusionné\n",
    "print(Dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7P_FPwDI0dIB",
    "outputId": "9403004b-3b74-4df7-c24b-9f21f137949d"
   },
   "outputs": [],
   "source": [
    "\"\"\"# Fusionner les datasets en utilisant les colonnes communes 'Libellé de la commune' et 'Date'\n",
    "Dataset = elections.merge(Niveau_education, on=['Libellé de la commune', 'Date']) \\\n",
    "                           .merge(revenu_final, on=['Libellé de la commune', 'Date']) \\\n",
    "                           .merge(Demographie, on=['Libellé de la commune', 'Date'])\n",
    "\n",
    "# Afficher la dataset fusionné\n",
    "Dataset\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pU_qok2c0dIC"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "encoder=LabelEncoder()\n",
    "\n",
    "# Créer une nouvelle colonne en concaténant les colonnes 'Nom' et 'Prénom'\n",
    "Dataset['Nom_complet'] = Dataset['Nom'] + ' ' + Dataset['Prénom']\n",
    "\n",
    "\n",
    "# Effectuer l'encodage des étiquettes dans la colonne 'Nom_complet'\n",
    "Dataset['Nom_complet']= le.fit_transform(Dataset['Nom_complet'])\n",
    "\n",
    "Dataset['Libellé de la commune']= encoder.fit_transform(Dataset['Libellé de la commune'])\n",
    "\n",
    "# Supprimer les colonnes 'Nom' et 'Prénom'\n",
    "Dataset.drop(['Nom', 'Prénom'], axis=1, inplace=True)\n",
    "\n",
    "##realisation du one-hot encoding pour les colonnes les features en chaine de caractères\n",
    "\n",
    "# Ajouter les colonnes  à la liste des colonnes à encoder en one-hot\n",
    "one_hot_encode_cols = ['Sexe']\n",
    "\n",
    "# Appliquer le one-hot encoding aux colonnes sélectionnées\n",
    "Dataset = pd.get_dummies(Dataset, columns=one_hot_encode_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhnIUw6L0dIC",
    "outputId": "10a0992f-ea17-401d-9479-e8577a3562cb"
   },
   "outputs": [],
   "source": [
    "# Transformation inverse de toutes les valeurs encodées dans la colonne 'Nom_complet'\n",
    "decoded_values = le.inverse_transform(Dataset['Nom_complet'])\n",
    "\n",
    "# Affichage des valeurs décodées\n",
    "print(decoded_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKaEoNDG0dIC"
   },
   "outputs": [],
   "source": [
    "# Créer une nouvelle colonne 'President' remplie de par prdant pour les candidats avec des voix inferieur a celui du vainqueur\n",
    "Dataset['President'] = \"perdant\"\n",
    "\n",
    "# Parcourir chaque date unique dans la colonne 'Date'\n",
    "for date in Dataset['Date'].unique():\n",
    "    # Parcourir chaque commune unique dans le dataset\n",
    "    for commune in Dataset['Libellé de la commune'].unique():\n",
    "        # Sélectionner les lignes correspondant à cette date et cette commune\n",
    "        subset = Dataset[(Dataset['Date'] == date) & (Dataset['Libellé de la commune'] == commune)]\n",
    "        # Trouver la valeur maximale dans la colonne 'Voix2' pour cette date et cette commune\n",
    "        max_value = subset['Voix2'].max()\n",
    "        # Mettre \"vainqueur\" dans la colonne 'President' pour les lignes ayant la valeur maximale\n",
    "        Dataset.loc[(Dataset['Date'] == date) & (Dataset['Libellé de la commune'] == commune) & (Dataset['Voix2'] == max_value), 'President'] = \"vainqueur\"\n",
    "        # Mettre \"perdant\" dans la colonne 'President' pour les lignes n'ayant pas la valeur maximale\n",
    "        Dataset.loc[(Dataset['Date'] == date) & (Dataset['Libellé de la commune'] == commune) & (Dataset['Voix2'] != max_value), 'President'] = \"perdant\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFGOZJLq0dIC"
   },
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "# Effectuer l'encodage des étiquettes dans la colonne 'Attack_type'\n",
    "Dataset[\"President\"] = le.fit_transform(Dataset[\"President\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVUEXj1I0dID"
   },
   "source": [
    "#Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEb7q7Hv0dID"
   },
   "outputs": [],
   "source": [
    "# Supprimer les colonnes 'Nom' et 'Prénom'\n",
    "Dataset.drop(['% Voix/Ins2', '% Voix/Exp2','Voix2'], axis=1, inplace=True)\n",
    "\n",
    "#Préparation des vécteurs\n",
    "X = Dataset.loc[:, \"Libellé de la commune\":\"President\"].drop(columns=[\"President\"])\n",
    "Y=Dataset[\"President\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXz0xAzr0dID"
   },
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "# Effectuer l'encodage des étiquettes dans la colonne 'Attack_type'\n",
    "Y = le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hat0gjMB0dID",
    "outputId": "ef0940b9-21cc-4964-afb1-1c79b2d848d4"
   },
   "outputs": [],
   "source": [
    "unique_values_Y = Dataset[\"President\"].unique()\n",
    "print(unique_values_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vN56QIN0dID",
    "outputId": "c07faac6-5894-49a1-fd1a-57c4f64ffe35"
   },
   "outputs": [],
   "source": [
    "# Importer la bibliothèque pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Calculer le décompte pour chaque catégorie dans la colonne \"President\"\n",
    "count_per_category = Dataset[\"President\"].value_counts()\n",
    "\n",
    "# Afficher le résultat\n",
    "print(count_per_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ict_8sl00dIE",
    "outputId": "5f2158e6-534f-407c-a0d5-15e43c69a03f"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Appliquer le StandardScaler aux caractéristiques numériques\n",
    "scaler = StandardScaler()\n",
    "X_standardiser = scaler.fit_transform(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fr_GvBFx0dIE",
    "outputId": "82bd2fff-32e6-4d65-9286-733462e0b274"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Supposons que X_standardiser est notre tableau NumPy contenant les données standardisées\n",
    "\n",
    "# Convertir X_standardiser en un DataFrame pandas avec des colonnes appropriées\n",
    "df_standardise = pd.DataFrame(X_standardiser)\n",
    "\n",
    "# Calculer la corrélation entre les variables\n",
    "corr = df_standardise.corr()\n",
    "\n",
    "# Tracer la heatmap de la corrélation\n",
    "sns.heatmap(data=corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdisnTs60dIE",
    "outputId": "c6648436-5a9d-487f-f90f-72c8a52392e4"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Création de l'objet PCA avec un nombre de composants approprié\n",
    "pca = PCA(n_components=15, svd_solver='full', random_state=1001)\n",
    "\n",
    "# Ajustement du PCA sur les données standardisées\n",
    "X_pca = pca.fit_transform(df_standardise)\n",
    "\n",
    "# Obtenez les noms des entités en sortie pour la transformation\n",
    "feature_names_out = [f\"PC{j+1}\" for j in range(X_pca.shape[1])]  # Créez les noms des composants principaux\n",
    "\n",
    "# Créer un DataFrame pandas avec les données transformées\n",
    "data_pca = pd.DataFrame(X_pca, columns=feature_names_out)\n",
    "\n",
    "# Afficher les données PCA\n",
    "print(data_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iriTGSGk0dIE",
    "outputId": "f563e5f7-f40c-4a00-f575-f07cce9a5a98"
   },
   "outputs": [],
   "source": [
    "print('Explained variance: %.4f' % pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZf5L-Gu0dIF"
   },
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Diviser les données en ensembles d'apprentissage et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_pca, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCq8bsfn0dIF"
   },
   "source": [
    "#Arbre de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7s1Vw9H0dIF",
    "outputId": "a7076b2f-e4d9-4a53-f121-423f4420c1c2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# modèle de Regression Logistique\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "#Rapport de classification\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bn0M1oUu0dIG",
    "outputId": "97c33d8c-f3a7-4048-a5c6-e6f1050290fb"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Supposez que y_true sont les vraies étiquettes et y_pred sont les prédictions du modèle\n",
    "# Calculer la matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Afficher la matrice de confusion sous forme graphique\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPYuIP4N0dIH"
   },
   "source": [
    "#Regression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cb9qXvGh0dIH",
    "outputId": "a361ebd9-a49c-4c24-f4f4-d41d178b8086"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# modèle de Regression Logistique\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "#Rapport de classification\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfV1KwF-0dIH",
    "outputId": "7a468415-fb2b-4955-9756-e2044a096672"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Supposez que y_true sont les vraies étiquettes et y_pred sont les prédictions du modèle\n",
    "# Calculer la matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Afficher la matrice de confusion sous forme graphique\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6P8_kov0dII"
   },
   "source": [
    "#KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8__gKPe0dII",
    "outputId": "e63062bc-db43-4a0a-ef96-84bbdd59dccc"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Définition du modèle ANN\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Évaluation du modèle\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calcul de l'exactitude\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Rapport de classification\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFprktfY0dIJ",
    "outputId": "f635ae78-e15c-40be-f26d-dd4458131cfd"
   },
   "outputs": [],
   "source": [
    "# Supposez que y_true sont les vraies étiquettes et y_pred sont les prédictions du modèle\n",
    "# Calculer la matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Afficher la matrice de confusion sous forme graphique\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wvH1DWirZFUj",
    "avGsOoIaLTHu",
    "wfHqIXM2tqCb"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
